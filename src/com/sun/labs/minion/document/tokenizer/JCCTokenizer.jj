/*
 *   Copyright 2007 Sun Microsystems, Inc. All rights reserved
 * 
 *   Use is subject to license terms.
 */
/**
 * A JavaCC implementation of the Universal Tokenizer.
 */ 

options {
  STATIC = false;
  CACHE_TOKENS = true;
}

PARSER_BEGIN(JCCTokenizer)
package com.sun.labs.minion.document.tokenizer;

import com.sun.labs.util.props.ConfigString;
import java.util.regex.Pattern;
import com.sun.labs.util.props.PropertyException;
import com.sun.labs.util.props.PropertySheet;
import com.sun.labs.minion.pipeline.Stage;
import com.sun.labs.minion.util.CharArrayReader;

public class JCCTokenizer extends Tokenizer {
    
    /**
     * A reusable reader for the characters that we'll be passed.
     */
    protected CharArrayReader reader;

    /**
     * A place to build up strings across tokens, if we need to.
     */
    protected StringBuilder buildUp;

    /**
     * Is the data that we've built up for an ngram tokenized language?
     */
    protected boolean isNgram;

    @ConfigString(defaultValue="")
    protected static String PROP_NO_BREAK_CHARS = "no_break_chars";

    /**
     * A regular expression pattern of characters for which we should not
     * break tokens.
     */
    protected Pattern noBreakChars;

    protected static String logTag = "JCCTOK";
    
    public JCCTokenizer() {
        this(null, false);
    }

    /**
     * Creates a JavaCC tokenizer that will not send punctuation to the downstream
     * stage.
     *
     * @param downstream the stage downstream of the tokenizer.
     */
    public JCCTokenizer(Stage downstream) {
        this(downstream, false);
    }

    /**
     * Creates a JavaCC tokenizer.
     * @param downstream the stage downstream of the tokenizer.
     * @param sendPunct if <code>true</code>, punctuation and whitespace will
     * be passed to the downstream stage.
     */
    public JCCTokenizer(Stage downstream, boolean sendPunct) {
        
        super(downstream, sendPunct);

        //
        // Stuff for this class.
        buildUp = new StringBuilder();
        reader = new CharArrayReader(new char[0], 0, 0);
        makeTokens = true;

        //
        // Set up the JavaCC stuff.  This is copied from the auto-generated stuff
        // because I want to use super above and because I need to create the
        // reader before I can hand it to these guys.
        jj_input_stream = new SimpleCharStream(reader, 1, 1);
        token_source = new JCCTokenizerTokenManager(jj_input_stream);
        token = new Token();
        token.next = jj_nt = token_source.getNextToken();
        jj_gen = 0;
        for (int i = 0; i < 1; i++) jj_la1[i] = -1;
  }

    public void text(char[] text, int b, int e) {

        //
        // Handle the text as field data.
        handleFieldData(text, b, e);
        
        //
        // If we're not making tokens, just build up this text.
        if(!makeTokens) {
            buildUp.append(text, b, e - b);
            return;
        }

        //
        // We need to tokenize this stuff.  Set the text in our reader and 
        // re-initialize the lexer.
        reader.reset(text, b, e);
        ReInit(reader);
        try {
            while(next());
        } catch (ParseException pe) {
            log.warn(logTag, 3, "Tokenizer parse exception", pe);
        }
    }

    public void handleLongChar(char c, int b, int l) {
        //
        // Tokenizing a single character is dumb, but we don't want to have to
        // recreate all of the rules!
        char[] temp = new char[1];
        temp[0] = c;
        reader.reset(temp, 0, 1);
        try {
            while(next());
        } catch (ParseException pe) {
            log.warn(logTag, 3, "Tokenizer parse exception", pe);
        }
    }

    public Tokenizer getTokenizer(Stage s, boolean sp) {
        return new JCCTokenizer(s, sp);
    }

    public void flush() {
        send();
    }

    protected void sendToken(String t, int type) {
       downstream.token(new com.sun.labs.minion.pipeline.Token(t, wordNum++, type, 0, 0, 1));
    }

    /**
     * Sends the built up token, if there is one.
     */
    public void send() {
        if(buildUp.length() == 0) {
            return;
        }
        
        //
        // We have ngram data.  Send it as overlapping bigrams.
        if(isNgram) {
            if(buildUp.length() == 1) {
                sendToken(buildUp.toString(), com.sun.labs.minion.pipeline.Token.BIGRAM);
            } else {
                for(int i = 0; i < buildUp.length() - 1; i++) {
                    sendToken(buildUp.substring(i, i+2), com.sun.labs.minion.pipeline.Token.BIGRAM);
                }
                sendToken(buildUp.charAt(buildUp.length() - 1) + "\0", com.sun.labs.minion.pipeline.Token.BIGRAM);
            }
        } else {
        
            //
            // Regular data.  Just send it.
            sendToken(buildUp.toString(), com.sun.labs.minion.pipeline.Token.NORMAL);
        }
        
        //
        // Clean up for the next send.
        buildUp.delete(0, buildUp.length());
   }

   public void setNoBreakChars(String nbcPattern) {
        if(!nbcPattern.equals("")) {
            try {
                noBreakChars = Pattern.compile(nbcPattern);
            } catch (java.util.regex.PatternSyntaxException pse) {
                log.warn(logTag, 2, "Error in noBreakChars pattern: " + nbcPattern,
                        pse);
                noBreakChars = null;
            }
        }
   }


    public void newProperties(PropertySheet ps) throws PropertyException {
        super.newProperties(ps);
        setNoBreakChars(ps.getString(PROP_NO_BREAK_CHARS));
    }
        
}
PARSER_END(JCCTokenizer)

/**
 * The following token rules are autogenerated by com.sun.labs.minion.document.tokenizer.JCCRules.
 * DO NOT HAND EDIT THEM!
 */ 
TOKEN : { < WHITESPACE : (<WHITECHAR>) (<WHITECHAR>)* > }
TOKEN : {
< #WHITECHAR : 
   "\u0020" | 
   "\u00a0" | 
   "\u1680" | 
   "\u180e" | 
   "\u2000" | 
   "\u2001" | 
   "\u2002" | 
   "\u2003" | 
   "\u2004" | 
   "\u2005" | 
   "\u2006" | 
   "\u2007" | 
   "\u2008" | 
   "\u2009" | 
   "\u200a" | 
   "\u200b" | 
   "\u2028" | 
   "\u2029" | 
   "\u202f" | 
   "\u205f" | 
   "\u3000"
> }
TOKEN : { < SPACESEPTOKEN : (<SPACESEPCHAR> | <SPACESEPCHAR>)+ > }
TOKEN: { < SPACESEPCHAR :
   (
    <SPACESEPCHAR1> | 
    <SPACESEPCHAR2> | 
    <SPACESEPCHAR3> | 
    <SPACESEPCHAR4> | 
    <SPACESEPCHAR5> | 
    <SPACESEPCHAR6> | 
    <SPACESEPCHAR7> | 
    <SPACESEPCHAR8> | 
    <SPACESEPCHAR9>
   )
> }
TOKEN : {
< #SPACESEPCHAR1 : 
   [
    "\u0030"-"\u0039",
    "\u0041"-"\u005a",
    "\u0061"-"\u007a",
    "\u00c0"-"\u00d6",
    "\u00d8"-"\u00f6",
    "\u00f8"-"\u0236",
    "\u0250"-"\u02c1",
    "\u02c6"-"\u02d1",
    "\u02e0"-"\u02e4",
    "\u0388"-"\u038a",
    "\u038e"-"\u03a1",
    "\u03a3"-"\u03ce",
    "\u03d0"-"\u03f5",
    "\u03f7"-"\u03fb",
    "\u0400"-"\u0481",
    "\u048a"-"\u04ce",
    "\u04d0"-"\u04f5",
    "\u04f8"-"\u04f9",
    "\u0500"-"\u050f",
    "\u0531"-"\u0556"
   ]
> }
TOKEN : {
< #SPACESEPCHAR2 : 
   [
    "\u0561"-"\u0587",
    "\u05d0"-"\u05ea",
    "\u05f0"-"\u05f2",
    "\u0712"-"\u072f",
    "\u074d"-"\u074f",
    "\u0780"-"\u07a5",
    "\u0904"-"\u0939",
    "\u0958"-"\u0961",
    "\u0966"-"\u096f",
    "\u0985"-"\u098c",
    "\u098f"-"\u0990",
    "\u0993"-"\u09a8",
    "\u09aa"-"\u09b0",
    "\u09b6"-"\u09b9",
    "\u09dc"-"\u09dd",
    "\u09df"-"\u09e1",
    "\u09e6"-"\u09f1",
    "\u0a05"-"\u0a0a",
    "\u0a0f"-"\u0a10",
    "\u0a13"-"\u0a28"
   ]
> }
TOKEN : {
< #SPACESEPCHAR3 : 
   [
    "\u0a2a"-"\u0a30",
    "\u0a32"-"\u0a33",
    "\u0a35"-"\u0a36",
    "\u0a38"-"\u0a39",
    "\u0a59"-"\u0a5c",
    "\u0a66"-"\u0a6f",
    "\u0a72"-"\u0a74",
    "\u0a85"-"\u0a8d",
    "\u0a8f"-"\u0a91",
    "\u0a93"-"\u0aa8",
    "\u0aaa"-"\u0ab0",
    "\u0ab2"-"\u0ab3",
    "\u0ab5"-"\u0ab9",
    "\u0ae0"-"\u0ae1",
    "\u0ae6"-"\u0aef",
    "\u0b05"-"\u0b0c",
    "\u0b0f"-"\u0b10",
    "\u0b13"-"\u0b28",
    "\u0b2a"-"\u0b30",
    "\u0b32"-"\u0b33"
   ]
> }
TOKEN : {
< #SPACESEPCHAR4 : 
   [
    "\u0b35"-"\u0b39",
    "\u0b5c"-"\u0b5d",
    "\u0b5f"-"\u0b61",
    "\u0b66"-"\u0b6f",
    "\u0d85"-"\u0d96",
    "\u0d9a"-"\u0db1",
    "\u0db3"-"\u0dbb",
    "\u0dc0"-"\u0dc6",
    "\u1000"-"\u1021",
    "\u1023"-"\u1027",
    "\u1029"-"\u102a",
    "\u1040"-"\u1049",
    "\u1050"-"\u1055",
    "\u1200"-"\u1206",
    "\u1208"-"\u1246",
    "\u124a"-"\u124d",
    "\u1250"-"\u1256",
    "\u125a"-"\u125d",
    "\u1260"-"\u1286",
    "\u128a"-"\u128d"
   ]
> }
TOKEN : {
< #SPACESEPCHAR5 : 
   [
    "\u1290"-"\u12ae",
    "\u12b2"-"\u12b5",
    "\u12b8"-"\u12be",
    "\u12c2"-"\u12c5",
    "\u12c8"-"\u12ce",
    "\u12d0"-"\u12d6",
    "\u12d8"-"\u12ee",
    "\u12f0"-"\u130e",
    "\u1312"-"\u1315",
    "\u1318"-"\u131e",
    "\u1320"-"\u1346",
    "\u1348"-"\u135a",
    "\u1369"-"\u1371",
    "\u13a0"-"\u13f4",
    "\u1401"-"\u166c",
    "\u166f"-"\u1676",
    "\u1681"-"\u169a",
    "\u16a0"-"\u16ea",
    "\u1700"-"\u170c",
    "\u170e"-"\u1711"
   ]
> }
TOKEN : {
< #SPACESEPCHAR6 : 
   [
    "\u1720"-"\u1731",
    "\u1740"-"\u1751",
    "\u1760"-"\u176c",
    "\u176e"-"\u1770",
    "\u1780"-"\u17b3",
    "\u17e0"-"\u17e9",
    "\u1810"-"\u1819",
    "\u1820"-"\u1877",
    "\u1880"-"\u18a8",
    "\u1900"-"\u191c",
    "\u1946"-"\u196d",
    "\u1970"-"\u1974",
    "\u1d00"-"\u1d6b",
    "\u1e00"-"\u1e9b",
    "\u1ea0"-"\u1ef9",
    "\u1f00"-"\u1f15",
    "\u1f18"-"\u1f1d",
    "\u1f20"-"\u1f45",
    "\u1f48"-"\u1f4d",
    "\u1f50"-"\u1f57"
   ]
> }
TOKEN : {
< #SPACESEPCHAR7 : 
   [
    "\u1f5f"-"\u1f7d",
    "\u1f80"-"\u1fb4",
    "\u1fb6"-"\u1fbc",
    "\u1fc2"-"\u1fc4",
    "\u1fc6"-"\u1fcc",
    "\u1fd0"-"\u1fd3",
    "\u1fd6"-"\u1fdb",
    "\u1fe0"-"\u1fec",
    "\u1ff2"-"\u1ff4",
    "\u1ff6"-"\u1ffc",
    "\u210a"-"\u2113",
    "\u2119"-"\u211d",
    "\u212a"-"\u212d",
    "\u212f"-"\u2131",
    "\u2133"-"\u2139",
    "\u213d"-"\u213f",
    "\u2145"-"\u2149",
    "\ufb00"-"\ufb06",
    "\ufb13"-"\ufb17",
    "\ufb1f"-"\ufb28"
   ]
> }
TOKEN : {
< #SPACESEPCHAR8 : 
   [
    "\ufb2a"-"\ufb36",
    "\ufb38"-"\ufb3c",
    "\ufb40"-"\ufb41",
    "\ufb43"-"\ufb44",
    "\ufb46"-"\ufb4f",
    "\uff10"-"\uff19",
    "\uff21"-"\uff3a",
    "\uff41"-"\uff5a"
   ]
> }
TOKEN : {
< #SPACESEPCHAR9 : 
   "\u00aa" | 
   "\u00b5" | 
   "\u00ba" | 
   "\u02ee" | 
   "\u037a" | 
   "\u0386" | 
   "\u038c" | 
   "\u0559" | 
   "\u0710" | 
   "\u07b1" | 
   "\u093d" | 
   "\u0950" | 
   "\u09b2" | 
   "\u09bd" | 
   "\u0a5e" | 
   "\u0abd" | 
   "\u0ad0" | 
   "\u0b3d" | 
   "\u0b71" | 
   "\u0dbd" | 
   "\u1248" | 
   "\u1258" | 
   "\u1288" | 
   "\u12b0" | 
   "\u12c0" | 
   "\u1310" | 
   "\u17d7" | 
   "\u17dc" | 
   "\u1f59" | 
   "\u1f5b" | 
   "\u1f5d" | 
   "\u1fbe" | 
   "\u2071" | 
   "\u207f" | 
   "\u2102" | 
   "\u2107" | 
   "\u2115" | 
   "\u2124" | 
   "\u2126" | 
   "\u2128" | 
   "\ufb1d" | 
   "\ufb3e"
> }
TOKEN : { < NGRAMTOKEN : (<NONSPACESEPCHAR>) (<NONSPACESEPCHAR>)* > }
TOKEN : {
< #NONSPACESEPCHAR : 
   [
    "\u0600"-"\u06ff",
    "\u0b80"-"\u0d7f",
    "\u0e00"-"\u0fbf",
    "\u10a0"-"\u11ff",
    "\u3001"-"\ud7ff",
    "\uf900"-"\ufaff",
    "\ufb50"-"\ufe4f",
    "\ufe70"-"\ufeff",
    "\uff60"-"\uffdf"
   ]
> }
TOKEN : { < PUNCTUATION : ~[] > }

/**
 * End of autogenerated rules.  Have a nice day.
 */

boolean next() :
{
  Token token = null;
}
{
  ( token = <SPACESEPTOKEN> |
    token = <NGRAMTOKEN> |
    token = <WHITESPACE> |
    token = <PUNCTUATION> |
    token = <EOF>
  )

  {
      switch(token.kind) {
         case EOF:
            //
            // This is the end of the reader.  If we're really at the end of the
            // file, flush() will call send().
            return false;
         case PUNCTUATION:
         case WHITESPACE:

            //
            // If we have a no breaking characters pattern, see if this token 
            // matches it.  If so, we'll collect the data and continue.
            if(noBreakChars != null &&
                noBreakChars.matcher(token.image).matches()) {
                buildUp.append(token.image);
                return true;
            }

            //
            // Send whatever's built up, since we're at a word boundary.
            send();

            //
            // If we're supposed to send punctuation, then send this token along
            // as well.  We don't care if punctuation gets broken up.
            if((token.kind == PUNCTUATION && sendPunct) ||
               (token.kind == WHITESPACE && sendWhite)) {
                downstream.punctuation(new com.sun.labs.minion.pipeline.Token(token.image,
                0, com.sun.labs.minion.pipeline.Token.PUNCT));
           }
            isNgram = false;
           return true;
         case NGRAMTOKEN:

            //
            // If we're not currently handling ngram tokenized data, we need to
            // send whatever we've built up and then set the ngram flag.
            if(!isNgram) {
                send();
            }

            //
            // Now we can start building up ngram tokenized data.
            isNgram = true;
            buildUp.append(token.image);
            return true;

         default:
            
            //
            // We're not in an ngram tokenized region.  If we have some built up
            // ngram stuff, we need to send it now.
            if(isNgram) {
                send();
            }

            // Add this to the string we're building.  If we hit some punctuation
            // or someone calls flush, we'll send the token.
            isNgram = false;
            buildUp.append(token.image);
            return true;
      }
  }
}

